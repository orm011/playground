{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orm/mambaforge/envs/playground2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from playground.load import clip_model, clip_processor, model_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute embedding first time: this will take a few minutes\n",
    "#from playground.load import dataset, extract_image_vectors\n",
    "#df = extract_image_vectors(dataset)\n",
    "#df.to_parquet('data/objectnet/embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load precomputed embeddings\n",
    "df = pd.read_parquet('data/objectnet/embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ys(df, label : str):\n",
    "    ''' get binary labels for a given class '''\n",
    "    assert label in df.label.unique()\n",
    "    return np.where(df.label == label, 1, 0)\n",
    "\n",
    "def get_text_embedding(text : str, prompt_template='A picture of a {}'):\n",
    "    ''' get CLIP vector representation of text query '''\n",
    "    text = text.replace('_', ' ')\n",
    "    text = prompt_template.format(text)\n",
    "    query_tokens = clip_processor(text=[text], return_tensors='pt')\n",
    "    query_vector = clip_model.get_text_features(query_tokens['input_ids'].to(model_device))\n",
    "    query_vector = F.normalize(query_vector)\n",
    "    query_vector = query_vector.cpu().detach().numpy().reshape(-1)\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8389910760803937"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = get_ys(df, 'banana')\n",
    "query_vector = get_text_embedding('banana')\n",
    "image_vectors = np.stack(df.vectors.values)\n",
    "image_scores = image_vectors @ query_vector\n",
    "average_precision_score(y_true, image_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with unit length vectors for dot product (works better for CLIP embeddings)\n",
    "df = df.assign(normalized_vectors=[vec for vec in np.stack(df['vectors']) / np.linalg.norm(df['vectors'])])\n",
    "\n",
    "# pick a random image from each class to be the query to be used as the positive example\n",
    "np.random.seed(10)\n",
    "df = df.assign(random_id=np.random.permutation(df.shape[0]))\n",
    "df = df.assign(group_rank=df.groupby('label')['random_id'].rank(method='first').astype('int'))\n",
    "df = df.assign(split=df.group_rank.apply(lambda x: 'query' if x <= 1 else 'test'))\n",
    "\n",
    "query_df = df[df.split == 'query']\n",
    "test_df = df[df.split == 'test']\n",
    "\n",
    "# from the test set, take a random sample of the DB which we will use as pseudo-negative examples\n",
    "# while training some of the linear models\n",
    "number_svm_train_examples = 2000\n",
    "random_sample = np.random.permutation(test_df.shape[0])[:number_svm_train_examples]\n",
    "Xneg = np.stack(test_df.iloc[random_sample].normalized_vectors.values)\n",
    "yneg = np.zeros(Xneg.shape[0])\n",
    "\n",
    "# the full test set used for evaluation\n",
    "Xtest = np.stack(test_df.normalized_vectors.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_from_text(row):\n",
    "    ''' get CLIP vector representation of text query, aka zero-shot search '''\n",
    "    return get_text_embedding(row.label)\n",
    "\n",
    "def get_vector_from_knn(row):\n",
    "    ''' get the vector representation of the row, aka nearest neighbor search '''\n",
    "    return row.normalized_vectors\n",
    "\n",
    "from sklearn import svm\n",
    "def get_vector_from_svm(row):\n",
    "    ''' ExemplarSVM: get the vector representation from using one positive example, and a random sample\n",
    "    labeled as negative, train using SVM and use this for the vector lookup '''\n",
    "    clf = svm.LinearSVC(class_weight='balanced', verbose=False, max_iter=10000, tol=1e-6, C=0.1)\n",
    "    Xpos = row.normalized_vectors.reshape(1, -1)\n",
    "    X = np.concatenate([Xpos, Xneg], axis=0)\n",
    "    y = np.concatenate([np.ones(1), yneg])\n",
    "    clf.fit(X, y) # train\n",
    "    return clf.coef_.reshape(-1)\n",
    "\n",
    "from sklearn import linear_model\n",
    "def get_vector_from_logistic_reg(row):\n",
    "    ''' Similar to ExemplarSVM, but using logistic regression instead '''\n",
    "    clf = linear_model.LogisticRegression(class_weight='balanced', fit_intercept=False, verbose=False, max_iter=10000, tol=1e-6, C=0.1)\n",
    "    Xpos = row.normalized_vectors.reshape(1, -1)\n",
    "    X = np.concatenate([Xpos, Xneg], axis=0)\n",
    "    y = np.concatenate([np.ones(1), yneg])\n",
    "    clf.fit(X, y) # train\n",
    "    return clf.coef_.reshape(-1)\n",
    "\n",
    "import importlib\n",
    "import playground.logistic_regression\n",
    "importlib.reload(playground.logistic_regression)\n",
    "from playground.logistic_regression import LinearModel\n",
    "\n",
    "def get_vector_svm_reg(row):\n",
    "    regularizer_vector  = get_vector_from_text(row)\n",
    "    clf = LinearModel(class_weight='balanced', label_loss_type='hinge_squared_loss', reg_norm_lambda=10.,\n",
    "                      verbose=False,\n",
    "                      regularizer_vector=regularizer_vector, reg_vector_lambda=1000.)\n",
    "    Xpos = row.normalized_vectors.reshape(1, -1)\n",
    "    X = np.concatenate([Xpos, Xneg], axis=0)\n",
    "    y = np.concatenate([np.ones(1), yneg])\n",
    "    clf.fit(X, y) # train\n",
    "    coeff = clf._module.weight.detach().cpu().numpy().reshape(-1)\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_method(query_df, vector_fn):\n",
    "    ''' run the evaluation for a given method over different categories on the dataset '''\n",
    "    aps = []\n",
    "    for (idx, row) in tqdm(query_df.iterrows(), total=query_df.shape[0]):\n",
    "        query_vector = vector_fn(row)\n",
    "        scores = Xtest @ query_vector\n",
    "        y_true = get_ys(test_df, row.label)\n",
    "        ap = average_precision_score(y_true, scores)\n",
    "        aps.append(ap)\n",
    "    return np.array(aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:16<00:00, 19.52it/s]\n"
     ]
    }
   ],
   "source": [
    "text_ap = eval_method(query_df, get_vector_from_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:35<00:00,  8.93it/s]\n"
     ]
    }
   ],
   "source": [
    "svm_ap = eval_method(query_df, get_vector_from_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:06<00:00, 46.19it/s]\n"
     ]
    }
   ],
   "source": [
    "knn_ap = eval_method(query_df, get_vector_from_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:47<00:00,  2.92it/s]\n"
     ]
    }
   ],
   "source": [
    "svm_reg_ap = eval_method(query_df, get_vector_svm_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svm_ap        0.095897\n",
       "text_ap       0.235935\n",
       "knn_ap        0.091348\n",
       "svm_reg_ap    0.253193\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_df = query_df.assign(svm_ap=svm_ap, text_ap=text_ap, knn_ap=knn_ap, svm_reg_ap=svm_reg_ap)\n",
    "query_df[['svm_ap', 'text_ap', 'knn_ap', 'svm_reg_ap']].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixeltable_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
