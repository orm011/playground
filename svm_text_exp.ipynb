{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('./data/objectnet/ground_truth/box_data.parquet')\n",
    "vecs = np.load('./data/objectnet/vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CLIPProcessor, CLIPModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
    "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = clip_processor(text=[\"a big banana\"], return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    query_vectors = clip_model.get_text_features(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(x):\n",
    "    return (clip_processor(images=[PIL.Image.open(x).convert('RGB')], return_tensors='pt'),\n",
    "            x.split('/')[-2],\n",
    "            '/'.join(x.split('/')[-2:])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/orm/mambaforge/envs/pixeltable_39/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/orm/mambaforge/envs/pixeltable_39/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'loader' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = torchvision.datasets.ImageFolder('./data/objectnet/images', loader=loader)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "output = []\n",
    "for batch in dataloader:\n",
    "    with torch.no_grad():\n",
    "        image_vectors = clip_model.get_image_features(batch['pixel_values'])\n",
    "    output.append(batch, image_vectors)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<PIL.Image.Image image mode=RGB size=224x224>,\n",
       "  'air_freshener/01d44a4b77b44a0.png',\n",
       "  'air_freshener'),\n",
       " 0)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    with torch.no_grad():\n",
    "        image_vectors = clip_model.get_image_features(batch[0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpixel_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "The [`CLIPModel`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "the latter silently ignores them.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Args:\n",
      "    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
      "        Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n",
      "        [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n",
      "    output_attentions (`bool`, *optional*):\n",
      "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "        tensors for more detail.\n",
      "    output_hidden_states (`bool`, *optional*):\n",
      "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "        more detail.\n",
      "    return_dict (`bool`, *optional*):\n",
      "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "    Returns:\n",
      "        image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n",
      "        applying the projection layer to the pooled output of [`CLIPVisionModel`].\n",
      "\n",
      "    Examples:\n",
      "\n",
      "    ```python\n",
      "    >>> from PIL import Image\n",
      "    >>> import requests\n",
      "    >>> from transformers import AutoProcessor, CLIPModel\n",
      "\n",
      "    >>> model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
      "    >>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
      "\n",
      "    >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
      "    >>> image = Image.open(requests.get(url, stream=True).raw)\n",
      "\n",
      "    >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "    >>> image_features = model.get_image_features(**inputs)\n",
      "    ```\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/envs/pixeltable_39/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "clip_processor(images=)\n",
    "clip_model.get_image_features(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>path</th>\n",
       "      <th>category</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>im_width</th>\n",
       "      <th>im_height</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>remote_control/5ffde0ea5aca4b3.png</td>\n",
       "      <td>remote control</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>remote_control/5ffde0ea5aca4b3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>remote_control/14bbfbb21fad46b.png</td>\n",
       "      <td>remote control</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>remote_control/14bbfbb21fad46b.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>remote_control/280ec1077fbf4af.png</td>\n",
       "      <td>remote control</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>remote_control/280ec1077fbf4af.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>remote_control/e9d445f6c153411.png</td>\n",
       "      <td>remote control</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>remote_control/e9d445f6c153411.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>remote_control/71f51802abb14f1.png</td>\n",
       "      <td>remote control</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>remote_control/71f51802abb14f1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50268</th>\n",
       "      <td>50268</td>\n",
       "      <td>combination_lock/3d48df5c7d7a44b.png</td>\n",
       "      <td>combination lock</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>combination_lock/3d48df5c7d7a44b.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50269</th>\n",
       "      <td>50269</td>\n",
       "      <td>combination_lock/d118e9b83ef04b7.png</td>\n",
       "      <td>combination lock</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>combination_lock/d118e9b83ef04b7.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50270</th>\n",
       "      <td>50270</td>\n",
       "      <td>combination_lock/5f424567488c4fc.png</td>\n",
       "      <td>combination lock</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>combination_lock/5f424567488c4fc.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50271</th>\n",
       "      <td>50271</td>\n",
       "      <td>combination_lock/120a2b5010e2493.png</td>\n",
       "      <td>combination lock</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>combination_lock/120a2b5010e2493.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50272</th>\n",
       "      <td>50272</td>\n",
       "      <td>combination_lock/3d0d0bcada14452.png</td>\n",
       "      <td>combination lock</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>combination_lock/3d0d0bcada14452.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50273 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                  path          category  x1   x2  \\\n",
       "0          0    remote_control/5ffde0ea5aca4b3.png    remote control   0  224   \n",
       "1          1    remote_control/14bbfbb21fad46b.png    remote control   0  224   \n",
       "2          2    remote_control/280ec1077fbf4af.png    remote control   0  224   \n",
       "3          3    remote_control/e9d445f6c153411.png    remote control   0  224   \n",
       "4          4    remote_control/71f51802abb14f1.png    remote control   0  224   \n",
       "...      ...                                   ...               ...  ..  ...   \n",
       "50268  50268  combination_lock/3d48df5c7d7a44b.png  combination lock   0  224   \n",
       "50269  50269  combination_lock/d118e9b83ef04b7.png  combination lock   0  224   \n",
       "50270  50270  combination_lock/5f424567488c4fc.png  combination lock   0  224   \n",
       "50271  50271  combination_lock/120a2b5010e2493.png  combination lock   0  224   \n",
       "50272  50272  combination_lock/3d0d0bcada14452.png  combination lock   0  224   \n",
       "\n",
       "       y1   y2  im_width  im_height                             file_path  \n",
       "0       0  224       224        224    remote_control/5ffde0ea5aca4b3.png  \n",
       "1       0  224       224        224    remote_control/14bbfbb21fad46b.png  \n",
       "2       0  224       224        224    remote_control/280ec1077fbf4af.png  \n",
       "3       0  224       224        224    remote_control/e9d445f6c153411.png  \n",
       "4       0  224       224        224    remote_control/71f51802abb14f1.png  \n",
       "...    ..  ...       ...        ...                                   ...  \n",
       "50268   0  224       224        224  combination_lock/3d48df5c7d7a44b.png  \n",
       "50269   0  224       224        224  combination_lock/d118e9b83ef04b7.png  \n",
       "50270   0  224       224        224  combination_lock/5f424567488c4fc.png  \n",
       "50271   0  224       224        224  combination_lock/120a2b5010e2493.png  \n",
       "50272   0  224       224        224  combination_lock/3d0d0bcada14452.png  \n",
       "\n",
       "[50273 rows x 10 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = vecs @ query_vectors.detach().numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.zeros(df.idx.max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[df[df.category == 'banana'].idx] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvecs = query_vectors.detach().numpy()\n",
    "qvecs = qvecs / np.linalg.norm(qvecs, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvec = qvecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = vecs @ qvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005697931859917126"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "average_precision_score(y_true, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40784, 38269, 27794, 30625, 30350, 10362, 49223, 17767, 35611,\n",
       "       10505])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-scores)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true[y_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixeltable_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
